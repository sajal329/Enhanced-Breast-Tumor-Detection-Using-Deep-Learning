{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2021025,"sourceType":"datasetVersion","datasetId":1209633},{"sourceId":301824,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":257764,"modelId":279041}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Adjust this path to match your own Kaggle environment/dataset path.\n# For example, if the dataset is in ../input/breast-ultrasound-images-dataset/\nBASE_PATH = \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT\"\n\n# We'll keep our data in a dictionary: { 'benign': [], 'malignant': [], 'normal': [] }\n# Each entry will be a list of tuples: (image_array, mask_array).\ndata = {\n    'benign': [],\n    'malignant': [],\n    'normal': []\n}\n\n# Helper function to load images and masks\ndef load_images_and_masks(folder_path):\n    \"\"\"\n    Given a folder path, returns a list of (image, mask) tuples.\n    It assumes every image has a corresponding mask file with '_mask' appended before the extension.\n    \"\"\"\n    # Gather all PNG files that are NOT masks\n    all_files = os.listdir(folder_path)\n    image_files = [f for f in all_files if f.endswith(\".png\") and \"_mask\" not in f]\n\n    loaded_data = []\n    for img_file in image_files:\n        img_path = os.path.join(folder_path, img_file)\n        \n        # Construct mask filename: \n        # e.g., if 'X.png' is the image, we expect 'X_mask.png' for the mask\n        mask_file = img_file.replace(\".png\", \"_mask.png\")\n        mask_path = os.path.join(folder_path, mask_file)\n\n        # Read the image and mask (grayscale or color depending on your need)\n        # Ultrasound is often grayscale, so let's do IMREAD_GRAYSCALE\n        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        if image is not None and mask is not None:\n            loaded_data.append((image, mask))\n        else:\n            print(f\"Warning: Could not read image/mask for {img_file}\")\n\n    return loaded_data\n\n# Loop through each subfolder and load data\nfor label in data.keys():\n    folder = os.path.join(BASE_PATH, label)\n    data[label] = load_images_and_masks(folder)\n\n# Now let's print a summary\nprint(\"Dataset Summary\")\nprint(\"---------------\")\ntotal_images = 0\nfor label in data.keys():\n    count = len(data[label])\n    total_images += count\n    # Print basic shape info for the first sample (just to give an idea)\n    if count > 0:\n        sample_img, sample_mask = data[label][0]\n        print(f\"Class: {label}\")\n        print(f\"  Number of samples: {count}\")\n        print(f\"  Sample image shape: {sample_img.shape}\")\n        print(f\"  Sample mask shape:  {sample_mask.shape}\")\n    else:\n        print(f\"Class: {label} - No images found.\")\n\nprint(\"---------------\")\nprint(f\"Total images loaded: {total_images}\")\n\n# Optional: Display a small example of an image + mask\n# Let's pick one from the 'benign' category (if it exists)\nif len(data['benign']) > 0:\n    sample_img, sample_mask = data['benign'][0]\n\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,2,1)\n    plt.imshow(sample_img, cmap='gray')\n    plt.title(\"Benign Sample Image\")\n    plt.axis('off')\n\n    plt.subplot(1,2,2)\n    plt.imshow(sample_mask, cmap='gray')\n    plt.title(\"Corresponding Mask\")\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Suppose you have a list of (image, mask) pairs for each class\nbenign_data = data['benign']  # List of (image, mask) tuples\nmalignant_data = data['malignant']\nnormal_data = data['normal']\n\n# Combine them and create labels\nX = benign_data + malignant_data + normal_data  # All image/mask pairs\ny = ([0]*len(benign_data)) + ([1]*len(malignant_data)) + ([2]*len(normal_data)) \n# 0 = benign, 1 = malignant, 2 = normal (or whichever labeling scheme you prefer)\n\n# First split into train+val and test\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    X, y, test_size=0.15, stratify=y, random_state=42\n)\n\n# Then split train+val into train and val\nX_train, X_val, y_train, y_val = train_test_split(\n    X_trainval, y_trainval, test_size=0.15, stratify=y_trainval, random_state=42\n)\n\nprint(f\"Train size: {len(X_train)}\")\nprint(f\"Val size:   {len(X_val)}\")\nprint(f\"Test size:  {len(X_test)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_image(img, size=(224, 224)):\n    # Resize\n    img_resized = cv2.resize(img, size)\n    # Normalize to 0-1\n    img_normalized = img_resized / 255.0\n    return img_normalized\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef preprocess_image(img, size=(224, 224)):\n    \"\"\"\n    Resizes the image to `size` and normalizes pixel values to [0,1].\n    For grayscale images, expand dims to get shape (H, W, 1).\n    \"\"\"\n    # Resize to the desired size\n    img_resized = cv2.resize(img, size)\n\n    # Convert to float and normalize to [0, 1]\n    img_normalized = img_resized.astype(np.float32) / 255.0\n\n    # If it's a single-channel image, expand dims to shape (224,224,1)\n    if len(img_normalized.shape) == 2:\n        img_normalized = np.expand_dims(img_normalized, axis=-1)\n\n    return img_normalized\n\n\n# Example label mapping\nlabel_map = {\n    'benign': 0,\n    'malignant': 1,\n    'normal': 2\n}\n\nX = []\ny = []\n\n# Iterate over each class label and its (image, mask) pairs\nfor label, pairs in data.items():\n    for (img, mask) in pairs:\n        # Preprocess the image\n        processed_img = preprocess_image(img, size=(224, 224))\n        \n        X.append(processed_img)\n        y.append(label_map[label])\n\n# Convert to numpy arrays\nX = np.array(X)\ny = np.array(y)\n\nprint(\"X shape:\", X.shape)  # (num_samples, 224, 224, 1) or (num_samples, 224, 224, 3)\nprint(\"y shape:\", y.shape)  # (num_samples,)\n\n\n# 1) Split train+val vs test\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    X, y,\n    test_size=0.15,\n    stratify=y,\n    random_state=42\n)\n\n# 2) Split train vs val\nX_train, X_val, y_train, y_val = train_test_split(\n    X_trainval, y_trainval,\n    test_size=0.15,  # 15% of (train+val)\n    stratify=y_trainval,\n    random_state=42\n)\n\nprint(\"Train size:\", len(X_train))\nprint(\"Validation size:\", len(X_val))\nprint(\"Test size:\", len(X_test))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data augmentation for the training set\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,        # rotate images by up to 20 degrees\n    width_shift_range=0.1,    # shift horizontally by 10%\n    height_shift_range=0.1,   # shift vertically by 10%\n    zoom_range=0.2,           # zoom in/out by 20%\n    horizontal_flip=True,     # flip horizontally\n    fill_mode='nearest'       # fill in missing pixels\n)\n\n# For validation and test, we usually do NOT augment, just the same normalization\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\nbatch_size = 32\n\ntrain_generator = train_datagen.flow(\n    X_train, y_train,\n    batch_size=batch_size,\n    shuffle=True\n)\n\nval_generator = val_datagen.flow(\n    X_val, y_val,\n    batch_size=batch_size,\n    shuffle=False\n)\n\ntest_generator = test_datagen.flow(\n    X_test, y_test,\n    batch_size=batch_size,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# vgg downloaded from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# For splitting data\nfrom sklearn.model_selection import train_test_split\n\n# For data augmentation and model building in Keras\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16, VGG19\nfrom tensorflow.keras import layers, models, optimizers\n\n################################################################################\n# 1. LOAD & PREPROCESS DATA\n################################################################################\n\n# Suppose you have a dictionary named `data` with keys: 'benign', 'malignant', 'normal'.\n# Each entry is a list of (image, mask) tuples, but for classification, we only need the image.\n# Example: data['benign'] = [(img1, mask1), (img2, mask2), ... ]\n\n# Map each label to a numeric class (0, 1, 2). Adjust if you prefer a different order.\nlabel_map = {\n    'benign': 0,\n    'malignant': 1,\n    'normal': 2\n}\n\ndef preprocess_image(img, size=(224, 224)):\n    \"\"\"\n    1) Resize to `size`.\n    2) Normalize to [0,1].\n    3) Convert grayscale -> 3 channels for VGG (if needed).\n    \"\"\"\n    # Resize\n    img_resized = cv2.resize(img, size)\n\n    # Normalize\n    img_resized = img_resized.astype(np.float32) / 255.0\n\n    # If single-channel (H,W), expand to (H,W,1) then tile to (H,W,3)\n    if len(img_resized.shape) == 2:  # grayscale\n        img_resized = np.expand_dims(img_resized, axis=-1)  # (H, W, 1)\n        img_resized = np.tile(img_resized, (1, 1, 3))       # (H, W, 3)\n\n    return img_resized\n\n# Gather images (X) and labels (y)\nX = []\ny = []\n\nfor label, pairs in data.items():\n    for (img, mask) in pairs:\n        processed_img = preprocess_image(img, size=(224, 224))\n        X.append(processed_img)\n        y.append(label_map[label])\n\nX = np.array(X)\ny = np.array(y)\n\nprint(\"Data shape:\", X.shape, \"Labels shape:\", y.shape)\n# Example: (780, 224, 224, 3)  (780,)\n\n################################################################################\n# 2. SPLIT DATA INTO TRAIN, VAL, TEST\n################################################################################\n\n# First, separate out a test set (15%), then a validation set (15% of the remainder)\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    X, y, test_size=0.15, stratify=y, random_state=42\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_trainval, y_trainval, test_size=0.15, stratify=y_trainval, random_state=42\n)\n\nprint(\"Train size:\", len(X_train))\nprint(\"Val size:\", len(X_val))\nprint(\"Test size:\", len(X_test))\n\n################################################################################\n# 3. DATA AUGMENTATION\n################################################################################\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# For validation and test, we typically only do rescaling (already normalized in preprocess)\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()\n\nbatch_size = 32\n\ntrain_generator = train_datagen.flow(\n    X_train, y_train,\n    batch_size=batch_size,\n    shuffle=True\n)\n\nval_generator = val_datagen.flow(\n    X_val, y_val,\n    batch_size=batch_size,\n    shuffle=False\n)\n\ntest_generator = test_datagen.flow(\n    X_test, y_test,\n    batch_size=batch_size,\n    shuffle=False\n)\n\n################################################################################\n# 4. BUILD VGG16 & VGG19 MODELS (TRANSFER LEARNING)\n################################################################################\n\ndef build_vgg16_model(input_shape=(224, 224, 3), num_classes=3, freeze_until=15):\n    \"\"\"\n    Build a VGG16 model (pretrained on ImageNet) for multi-class classification.\n    freeze_until: number of layers to freeze in the base model (set to None to unfreeze all).\n    \"\"\"\n    local_weights_path = \"/kaggle/input/vgg16/keras/default/1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n    \n    base_model = VGG16(\n        weights=local_weights_path,\n        include_top=False,\n        input_shape=input_shape\n    )\n    \n    # Freeze the first `freeze_until` layers\n    if freeze_until is not None:\n        for layer in base_model.layers[:freeze_until]:\n            layer.trainable = False\n    \n    # Build classification head\n    x = layers.Flatten()(base_model.output)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    model = models.Model(inputs=base_model.input, outputs=outputs)\n    return model\n\ndef build_vgg19_model(input_shape=(224, 224, 3), num_classes=3, freeze_until=17):\n    \"\"\"\n    Build a VGG19 model (pretrained on ImageNet) for multi-class classification.\n    freeze_until: number of layers to freeze in the base model (set to None to unfreeze all).\n    \"\"\"\n    local_weights_path = \"/kaggle/input/vgg16/keras/default/1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n    \n    base_model = VGG16(\n        weights=local_weights_path,\n        include_top=False,\n        input_shape=input_shape\n    )\n    \n    # Freeze the first `freeze_until` layers\n    if freeze_until is not None:\n        for layer in base_model.layers[:freeze_until]:\n            layer.trainable = False\n    \n    # Build classification head\n    x = layers.Flatten()(base_model.output)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    model = models.Model(inputs=base_model.input, outputs=outputs)\n    return model\n\n################################################################################\n# 5. TRAIN & EVALUATE VGG16\n################################################################################\n\nvgg16_model = build_vgg16_model(input_shape=(224, 224, 3), num_classes=3, freeze_until=15)\nvgg16_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nprint(\"\\n=== VGG16 Model Summary ===\")\nvgg16_model.summary()\n\n# Train\nhistory_vgg16 = vgg16_model.fit(\n    train_generator,\n    epochs=5,  # increase for better results\n    validation_data=val_generator\n)\n\n# Evaluate on test set\ntest_loss_vgg16, test_acc_vgg16 = vgg16_model.evaluate(test_generator)\nprint(\"VGG16 Test Accuracy:\", test_acc_vgg16)\n\n################################################################################\n# 6. TRAIN & EVALUATE VGG19\n################################################################################\n\nvgg19_model = build_vgg19_model(input_shape=(224, 224, 3), num_classes=3, freeze_until=17)\nvgg19_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nprint(\"\\n=== VGG19 Model Summary ===\")\nvgg19_model.summary()\n\n# Train\nhistory_vgg19 = vgg19_model.fit(\n    train_generator,\n    epochs=5,  # increase for better results\n    validation_data=val_generator\n)\n\n# Evaluate on test set\ntest_loss_vgg19, test_acc_vgg19 = vgg19_model.evaluate(test_generator)\nprint(\"VGG19 Test Accuracy:\", test_acc_vgg19)\n\n################################################################################\n# 7. COMPARE RESULTS\n################################################################################\n\nprint(\"\\nFinal Results:\")\nprint(f\"VGG16 Test Accuracy: {test_acc_vgg16:.4f}\")\nprint(f\"VGG19 Test Accuracy: {test_acc_vgg19:.4f}\")\n\n################################################################################\n# 8. OPTIONAL: CONFUSION MATRIX & CLASSIFICATION REPORT\n################################################################################\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Predict classes for VGG16\npreds_vgg16 = vgg16_model.predict(test_generator)\npred_classes_vgg16 = np.argmax(preds_vgg16, axis=1)\ntrue_classes = y_test  # Use the original test labels\n\ncm_vgg16 = confusion_matrix(true_classes, pred_classes_vgg16)\nprint(\"\\n=== VGG16 Confusion Matrix ===\")\nprint(cm_vgg16)\n\nprint(\"\\n=== VGG16 Classification Report ===\")\nprint(classification_report(true_classes, pred_classes_vgg16, target_names=[\"Benign\", \"Malignant\", \"Normal\"]))\n\n# Predict classes for VGG19\npreds_vgg19 = vgg19_model.predict(test_generator)\npred_classes_vgg19 = np.argmax(preds_vgg19, axis=1)\n\ncm_vgg19 = confusion_matrix(true_classes, pred_classes_vgg19)\nprint(\"\\n=== VGG19 Confusion Matrix ===\")\nprint(cm_vgg19)\n\nprint(\"\\n=== VGG19 Classification Report ===\")\nprint(classification_report(true_classes, pred_classes_vgg19, target_names=[\"Benign\", \"Malignant\", \"Normal\"]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DeepNet","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}